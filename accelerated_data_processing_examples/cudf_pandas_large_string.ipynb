{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msaharan/.github/blob/main/accelerated_data_processing_examples/cudf_pandas_large_string.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84635d55-68a2-468b-ac09-9029ebdab55f",
      "metadata": {
        "id": "84635d55-68a2-468b-ac09-9029ebdab55f"
      },
      "source": [
        "# Accelerating large string data processing with cudf pandas accelerator mode (cudf.pandas)\n",
        "<a href=\"https://github.com/rapidsai/cudf\">cuDF</a> is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating tabular data using a DataFrame style API in the style of pandas.\n",
        "\n",
        "cuDF now provides a <a href=\"https://rapids.ai/cudf-pandas/\">pandas accelerator mode</a> (`cudf.pandas`), allowing you to bring accelerated computing to your pandas workflows without requiring any code change.\n",
        "\n",
        "This notebook demonstrates how cuDF pandas accelerator mode can help accelerate processing of datasets with large string fields (4 GB+) processing by simply adding a `%load_ext` command. We have introduced this feature as part of our Rapids 24.08 release."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb8fe7ab-c055-40e9-897d-c62c72f28a16",
      "metadata": {
        "id": "bb8fe7ab-c055-40e9-897d-c62c72f28a16"
      },
      "source": [
        "# ⚠️ Verify your setup\n",
        "\n",
        "First, we'll verify that you are running with an NVIDIA GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a88b8586-cfdd-4d31-9b4d-9be8508f7ba0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a88b8586-cfdd-4d31-9b4d-9be8508f7ba0",
        "outputId": "67ed2484-e3b6-4ecf-b5aa-227c7d12ecce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  1 18:24:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi  # this should display information about available GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd58071-4371-428b-8a02-9cd66e6cb91f",
      "metadata": {
        "id": "5cd58071-4371-428b-8a02-9cd66e6cb91f"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb67713-7cf4-415a-bce7-ff4695862faa",
      "metadata": {
        "id": "9eb67713-7cf4-415a-bce7-ff4695862faa"
      },
      "source": [
        "## Overview\n",
        "The data we'll be working with summarizes job postings data that a developer working at a job listing firm might analyze to understand posting trends.\n",
        "\n",
        "We'll need to download a curated copy of this Kaggle dataset [https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024/data?select=job_summary.csv] directly from the kaggle API.  \n",
        "\n",
        "**Data License and Terms** <br>\n",
        "As this dataset originates from a Kaggle dataset, it's governed by that dataset's license and terms of use, which is the Open Data Commons license. Review here: https://opendatacommons.org/licenses/by/1-0/index.html . For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose.\n",
        "\n",
        "**Are there restrictions on how I can use this data? </br>**\n",
        "For each dataset an user elects to use, the user is responsible for checking if the dataset license is fit for the intended purpose.\n",
        "\n",
        "## Get the Data\n",
        "First, [please follow these instructions from Kaggle to download and/or updating your Kaggle API token to get acces the dataset](https://www.kaggle.com/discussions/general/74235).  \n",
        "- If you're using Colab, you can skip Step #1\n",
        "- If you're working on your local system, you can skip the Step #2.\n",
        "\n",
        "This should take about 1-2 minutes.\n",
        "\n",
        "Next, run this code below, which should also take 1-2 minutes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3efacb3c-5f3d-4ff0-b32a-76bbb80b5f74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3efacb3c-5f3d-4ff0-b32a-76bbb80b5f74",
        "outputId": "a234df78-cb08-4e5c-b6f3-716ad8b3c47a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "unzip:  cannot find or open 1-3m-linkedin-jobs-and-skills-2024.zip, 1-3m-linkedin-jobs-and-skills-2024.zip.zip or 1-3m-linkedin-jobs-and-skills-2024.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset through kaggle API-\n",
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
        "#unzip the file to access contents\n",
        "!unzip 1-3m-linkedin-jobs-and-skills-2024.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cbf10b-a540-4346-938d-0abbdc0d4524",
      "metadata": {
        "id": "26cbf10b-a540-4346-938d-0abbdc0d4524"
      },
      "source": [
        "# Analysis using Standard Pandas\n",
        "\n",
        "First, let's use Pandas to read in some columns of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af373e43-3815-4c90-a5ef-9a32f76a908c",
      "metadata": {
        "id": "af373e43-3815-4c90-a5ef-9a32f76a908c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd"
      ],
      "metadata": {
        "id": "XpxvrqZ3qCec",
        "outputId": "222673ff-14ce-4e7f-a6b0-b449b36508ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XpxvrqZ3qCec",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'pandas' from '/usr/local/lib/python3.12/dist-packages/pandas/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314fbc21-a62f-442e-bb75-432cdd7e2e71",
      "metadata": {
        "id": "314fbc21-a62f-442e-bb75-432cdd7e2e71"
      },
      "source": [
        "**Job Summary Dataset** (Dataset-1) : This dataset contains job summaries for each job link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "db3ed82c-0d80-4044-98f8-1e61b5548c9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "db3ed82c-0d80-4044-98f8-1e61b5548c9d",
        "outputId": "f1eecd91-63a3-46f4-8531-b49ea5cdcfc6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'job_summary.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'job_summary.csv'"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "job_summary_df = pd.read_csv(\"job_summary.csv\", dtype=('str'))\n",
        "print(\"Dataset Size (in GB):\",round(job_summary_df.memory_usage(\n",
        "    deep=True).sum()/(1024**3),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b2dc7e9-bf6e-4566-abd5-e64530614279",
      "metadata": {
        "id": "3b2dc7e9-bf6e-4566-abd5-e64530614279"
      },
      "source": [
        "This **8 GB (in-memory)** dataset took around 1 minute to load!\n",
        "\n",
        "Let's examine the dataset entries along with their memory footprint and character length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "888f3aaa-8ceb-4370-aabf-78f65b18c3b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "888f3aaa-8ceb-4370-aabf-78f65b18c3b0",
        "outputId": "01f40722-af7e-4ae1-df78-551a625d9bfd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'job_summary_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-778921294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob_summary_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'job_summary_df' is not defined"
          ]
        }
      ],
      "source": [
        "job_summary_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0a7b95-c8b2-4e35-86d5-ce888c7242fe",
      "metadata": {
        "id": "7f0a7b95-c8b2-4e35-86d5-ce888c7242fe"
      },
      "source": [
        "The dataset contains job summaries from various job listings.\n",
        "\n",
        "The `job_summary` column is particularly large, occupying **5 GB in size with a total of 5 billion characters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61cd04b6-fdfd-4c80-9d32-c1e78bf2f292",
      "metadata": {
        "id": "61cd04b6-fdfd-4c80-9d32-c1e78bf2f292"
      },
      "outputs": [],
      "source": [
        "# Calculate memory usage of each column in GB\n",
        "memory_usage_bytes = job_summary_df.memory_usage(deep=True)\n",
        "memory_usage_gb = memory_usage_bytes / (1024 ** 3)\n",
        "\n",
        "print(\"`job_summary` column size (in GB):\", round(memory_usage_gb['job_summary'],1),\n",
        "     \"\\n\",\"`job_summary` column number of characters (in Bn):\",\n",
        "      round(job_summary_df['job_summary'].str.len().sum()/(10**9),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa298f0-e735-42fa-a6ce-935df7ec9dcb",
      "metadata": {
        "id": "4fa298f0-e735-42fa-a6ce-935df7ec9dcb"
      },
      "source": [
        "**Job Skills Dataset** (Dataset-2): This dataset contains a mapping between job links and the skill tags associated with the link."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd3a366-c3ff-4d03-8c3e-7701b6ac16cd",
      "metadata": {
        "id": "8fd3a366-c3ff-4d03-8c3e-7701b6ac16cd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "job_skills_df = pd.read_csv(\"job_skills.csv\", dtype=('str'))\n",
        "job_skills_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3e6236-048d-4f1a-b1a8-f94356e2e93a",
      "metadata": {
        "id": "aa3e6236-048d-4f1a-b1a8-f94356e2e93a"
      },
      "outputs": [],
      "source": [
        "job_skills_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8667bd9-cc6d-4e75-8d4f-c47d7756c3d0",
      "metadata": {
        "id": "e8667bd9-cc6d-4e75-8d4f-c47d7756c3d0"
      },
      "source": [
        "**Job Postings Dataset** (Dataset - 3): This contains demographic and other work related details for each job posting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399ce5b4-e38c-49ac-b6fd-c572169f933b",
      "metadata": {
        "id": "399ce5b4-e38c-49ac-b6fd-c572169f933b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "job_postings_df = pd.read_csv(\"linkedin_job_postings.csv\", dtype=('str'))\n",
        "job_postings_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b54b6f-24bf-4152-a2e5-55e14acd6060",
      "metadata": {
        "id": "15b54b6f-24bf-4152-a2e5-55e14acd6060"
      },
      "outputs": [],
      "source": [
        "job_postings_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43b86e1-1e64-4d4d-a07e-29066c5d362d",
      "metadata": {
        "id": "a43b86e1-1e64-4d4d-a07e-29066c5d362d"
      },
      "source": [
        "## Q. Which companies and roles have extremely long job summary?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29befdfc-6794-4abc-9694-e8b702995063",
      "metadata": {
        "id": "29befdfc-6794-4abc-9694-e8b702995063"
      },
      "source": [
        "Long job summaries can be challenging to read, but they are essential for certain roles requiring specific subject matter expertise. It would be interesting to identify which job roles and companies have extremely long summaries.\n",
        "\n",
        "Let's determine the length of each job summary using the `.str.len()` method in pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fabc85fa-7e61-45f2-8716-8c6dcf4d18b1",
      "metadata": {
        "id": "fabc85fa-7e61-45f2-8716-8c6dcf4d18b1"
      },
      "outputs": [],
      "source": [
        "#Calculate Length of job summary -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdeb75d0-baa1-401e-b1e3-69edd42fb43f",
      "metadata": {
        "id": "bdeb75d0-baa1-401e-b1e3-69edd42fb43f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "job_summary_df['summary_length'] = job_summary_df['job_summary'].str.len()\n",
        "job_summary_df['summary_length'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4652afd0-4776-46a3-ae1e-93a2b56a0ae0",
      "metadata": {
        "id": "4652afd0-4776-46a3-ae1e-93a2b56a0ae0"
      },
      "source": [
        "To identify job roles and companies with the longest job summaries, we need to merge the two datasets using the `job_link` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32baa2d2-b18b-403e-af22-d5b37f5ae508",
      "metadata": {
        "id": "32baa2d2-b18b-403e-af22-d5b37f5ae508"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_merged=pd.merge(job_postings_df, job_summary_df, how=\"left\", on=\"job_link\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e475852-55b3-40ff-9fbd-c66c243cc227",
      "metadata": {
        "id": "0e475852-55b3-40ff-9fbd-c66c243cc227"
      },
      "source": [
        "Lets finally look at the `job_tile` and `company` with the maximum job summary length through data aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a757c781-41a3-4c2b-937e-8dc3f4a72331",
      "metadata": {
        "id": "a757c781-41a3-4c2b-937e-8dc3f4a72331"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_merged.groupby(['company',\"job_title\"]).agg({\n",
        "    \"summary_length\":\"mean\"}).sort_values(by='summary_length', ascending = False).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784389e3-2ece-4f29-a9f4-025be7a6a20b",
      "metadata": {
        "id": "784389e3-2ece-4f29-a9f4-025be7a6a20b"
      },
      "source": [
        "We see that some specialized jobs like `Adoloscent Behavioural Therapist`, & `Airside Project Manager` have longer summary length."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cbb978-3d18-461f-b430-8936d58ae542",
      "metadata": {
        "id": "94cbb978-3d18-461f-b430-8936d58ae542"
      },
      "source": [
        "## Q. How does the length of job summary varies by location?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352346b6-b4f5-4da2-8d03-81f9d1f3d447",
      "metadata": {
        "id": "352346b6-b4f5-4da2-8d03-81f9d1f3d447"
      },
      "source": [
        "Why stop here?\n",
        "\n",
        "Another interesting trend would be to see whether job summary length changes with location. Hopefully, the role requirements shouldn't be biased by the location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2851c556-a4e3-47a0-bacc-be9368e5e876",
      "metadata": {
        "id": "2851c556-a4e3-47a0-bacc-be9368e5e876"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Group by company, job_title, and month, and calculate the mean of summary_length\n",
        "grouped_df = df_merged.groupby(['job_title', 'job_location']).agg({'summary_length': 'mean'})\n",
        "\n",
        "# Reset index to sort by job_title and month\n",
        "grouped_df = grouped_df.reset_index()\n",
        "\n",
        "# Sort by job_title and month\n",
        "sorted_df = grouped_df.sort_values(by=['job_title', 'job_location','summary_length'],\n",
        "                                   ascending=False).reset_index(drop=True).fillna(0)\n",
        "sorted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d2cd50-f58f-4703-9df9-5e229ab1bd77",
      "metadata": {
        "id": "90d2cd50-f58f-4703-9df9-5e229ab1bd77"
      },
      "source": [
        "Let's analyze the job role `LEAD SALES ASSOCIATE-FT` to see if the job summary changes with its postings across different locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1d4e3b-8551-488d-864a-d94059959941",
      "metadata": {
        "id": "9d1d4e3b-8551-488d-864a-d94059959941"
      },
      "outputs": [],
      "source": [
        "# isolating records for the specific job role across various location\n",
        "job_title_acc=sorted_df[sorted_df['job_title'] == 'LEAD SALES ASSOCIATE-FT'].reset_index(\n",
        "    drop=True)[1:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f31c1f5-e6a0-4195-8972-f33e0889ae66",
      "metadata": {
        "id": "9f31c1f5-e6a0-4195-8972-f33e0889ae66",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7, 3.5))\n",
        "plt.barh(job_title_acc['job_location'],job_title_acc['summary_length'], color='skyblue')\n",
        "plt.xlabel('Summary Length')\n",
        "plt.ylabel('Job Title')\n",
        "plt.title('job summary length (Lead Sales Associate) role across cities')\n",
        "plt.gca().invert_yaxis()  # To display the highest values at the top\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614f1ffc-72bf-46a7-9b0a-999ddbb0060a",
      "metadata": {
        "id": "614f1ffc-72bf-46a7-9b0a-999ddbb0060a"
      },
      "source": [
        "The `Lead Sales Associate` role has similar job summary lengths across various cities, indicating that job requirements for this role remain consistent regardless of location."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2__ZMVe6LaBJ",
      "metadata": {
        "id": "2__ZMVe6LaBJ"
      },
      "source": [
        "# Analysis with cuDF Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38aee6bb-46d8-4a2f-b0d5-daf86b12356a",
      "metadata": {
        "id": "38aee6bb-46d8-4a2f-b0d5-daf86b12356a"
      },
      "source": [
        "Typically, you should load the `cudf.pandas` extension as the first step in your notebook, before importing any modules. Here, we explicitly restart the kernel to simulate that behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19200808-91c2-4dda-ada4-ee5e4d2fcf31",
      "metadata": {
        "id": "19200808-91c2-4dda-ada4-ee5e4d2fcf31"
      },
      "outputs": [],
      "source": [
        "get_ipython().kernel.do_shutdown(restart=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18cc766c-5631-49ec-bb1e-f566763b2e13",
      "metadata": {
        "id": "18cc766c-5631-49ec-bb1e-f566763b2e13"
      },
      "source": [
        "Note: We just added the `%load-ext` and the rest of the code remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eadb8d77-cb45-4c7c-ae9f-77e47a4f29b3",
      "metadata": {
        "id": "eadb8d77-cb45-4c7c-ae9f-77e47a4f29b3"
      },
      "outputs": [],
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd"
      ],
      "metadata": {
        "id": "dBvlZhf6qFWw"
      },
      "id": "dBvlZhf6qFWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "196268f2-6169-4ed7-a9e6-db9078caa6ab",
      "metadata": {
        "id": "196268f2-6169-4ed7-a9e6-db9078caa6ab"
      },
      "source": [
        "We'll run the same code as above to get a feel what GPU-acceleration brings to pandas workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3b6a16-ff72-4421-b43c-06c33f57ec12",
      "metadata": {
        "id": "ae3b6a16-ff72-4421-b43c-06c33f57ec12"
      },
      "outputs": [],
      "source": [
        "%time job_summary_df = pd.read_csv(\"job_summary.csv\", dtype=('str'))\n",
        "print(\"Dataset Size (in GB):\",round(job_summary_df.memory_usage(\n",
        "    deep=True).sum()/(1024**3),2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c506e1-f135-4afb-8fc7-23e72c05d73c",
      "metadata": {
        "id": "01c506e1-f135-4afb-8fc7-23e72c05d73c"
      },
      "source": [
        "The same dataset takes about around 1.5 minutes to load with pandas. That's around **5x speedup** with no changes to the code!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d0a0e1-1d74-494d-bd12-b829f11eeede",
      "metadata": {
        "id": "d9d0a0e1-1d74-494d-bd12-b829f11eeede"
      },
      "source": [
        "Let's load the remaining two datasets as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e4cf7e-8824-4822-9d30-46b81ba2acd7",
      "metadata": {
        "id": "12e4cf7e-8824-4822-9d30-46b81ba2acd7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "job_skills_df = pd.read_csv(\"job_skills.csv\", dtype=('str'))\n",
        "job_postings_df = pd.read_csv(\"linkedin_job_postings.csv\", dtype=('str'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c8f9da-121f-4311-8a79-274425363e5e",
      "metadata": {
        "id": "13c8f9da-121f-4311-8a79-274425363e5e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "job_summary_df['summary_length'] = job_summary_df['job_summary'].str.len()\n",
        "job_summary_df['summary_length'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b68792-5c64-4ebd-9d80-cf6ff55baeef",
      "metadata": {
        "id": "67b68792-5c64-4ebd-9d80-cf6ff55baeef"
      },
      "source": [
        "That was lightning fast! We went from around 10+ (with pandas) to a few milliseconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e1cc84-debb-4da7-bc20-5c7139f786f7",
      "metadata": {
        "id": "31e1cc84-debb-4da7-bc20-5c7139f786f7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_merged=pd.merge(job_postings_df, job_summary_df, how=\"left\", on=\"job_link\")\n",
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0160a559-2b17-40a6-ad9d-34ce746236d0",
      "metadata": {
        "id": "0160a559-2b17-40a6-ad9d-34ce746236d0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_merged.groupby(['company',\"job_title\"]).agg({\n",
        "    \"summary_length\":\"mean\"}).sort_values(by='summary_length', ascending = False).fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IME4urGYQ3qS",
      "metadata": {
        "id": "IME4urGYQ3qS"
      },
      "source": [
        "We went down from around 5 seconds to less than a second here. This is in line with our speedups on other operations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adc00726-f151-41f4-8731-a1ce1f83eea2",
      "metadata": {
        "id": "adc00726-f151-41f4-8731-a1ce1f83eea2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Group by company, job_title, and month, and calculate the mean of summary_length\n",
        "grouped_df = df_merged.groupby(['job_title', 'job_location']).agg({'summary_length': 'mean'})\n",
        "\n",
        "# Reset index to sort by job_title and month\n",
        "grouped_df = grouped_df.reset_index()\n",
        "\n",
        "# Sort by job_title and month\n",
        "sorted_df = grouped_df.sort_values(by=['job_title', 'job_location','summary_length'],\n",
        "                                   ascending=False).reset_index(drop=True).fillna(0)\n",
        "sorted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c97b81-64c5-48fb-8fe0-d36789cf3deb",
      "metadata": {
        "id": "08c97b81-64c5-48fb-8fe0-d36789cf3deb"
      },
      "source": [
        "The acceleration is consistently 10x+ for complex aggregations and sorting that involve multiple columns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bcc719b-666a-4bc9-97d6-16f448b5c707",
      "metadata": {
        "id": "9bcc719b-666a-4bc9-97d6-16f448b5c707"
      },
      "source": [
        "# Summary\n",
        "\n",
        "With cudf.pandas, you can keep using pandas as your primary dataframe library. When things start to get a little slow, just load the `cudf.pandas` extension and enjoy the incredible speedups.\n",
        "\n",
        "If you like Google Colab and want to get peak `cudf.pandas` performance to process even larger datasets, Google Colab's paid tier includes both L4 and A100 GPUs (in addition to the T4 GPU this demo notebook is using).\n",
        "\n",
        "To learn more about cudf.pandas, we encourage you to visit https://rapids.ai/cudf-pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76bfe45d-dee5-435a-86ce-e3c945692a40",
      "metadata": {
        "id": "76bfe45d-dee5-435a-86ce-e3c945692a40"
      },
      "source": [
        "# Do you have any feedback for us?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5ad763-961a-453b-ab65-f95c4b8c78df",
      "metadata": {
        "id": "0d5ad763-961a-453b-ab65-f95c4b8c78df"
      },
      "source": [
        "Fill this quick survey <a href=\"https://www.surveymonkey.com/r/TX3QQQR\">HERE</a>\n",
        "\n",
        "Raise an issue on our github repo <a href=\"https://github.com/rapidsai/cudf/issues\">HERE</a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python_cudf_largestring",
      "language": "python",
      "name": "rapids_large_string"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}